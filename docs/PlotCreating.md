# Plot Creating

This file describes how benchmark plots are created.

## Plot script definition

Special plot script is responsible for creating benchmark plots. It is a Python3 script defined in the file `/ADBench/plot_graphs.py`. It has the following requirements:

* matplotlib
* plotly

This script looks through the directory `/tmp` and for each its subdirectory, that contains [global runner](Architecture.md#Global-Runner) results, creates graphs storing them to the folder `/tmp/graphs`. Two kinds of graphs are created: static PNG files and dynamic [plotly](https://plot.ly/python/) HTML files.

The script command line arguments are:

| Argument | Definition |
| -- | -- |
| `--save` | If specified then the script saves static PNG files contain created plots |
| `--plotly` | If specified then the script saves plotly dynamic HTML files contain created plots |
| `--show` | If specified then the script shows created static plots on the screen |
| `-h` or `--help` or `-?` | Getting reference about the script CMD arguments |

__Note__: if neither `--save` nor `--plotly` is specified then `--show` is used by default.

## Input files

The script processes information from the files generated by the global runner that are stored in the directory `tmp/<BuildType>`. Here `<BuildType>` is a build type of the benchmark run (most often `Release`). This folder consists of several subfolders that contain information for different objective types (`ba`, `gmm` etc.). In each of these subfolders there are different tool directories that contain calculated result files, correctness files and benchmark time files. The plot script uses info from the correctness and the time files to create graphs. Time file names have the following form:

```
<DataFileName>_times_<ToolName>.txt
```

Here `<DataFileName>` is the name of the data input file that is used for run result calculating. `<ToolName>` part of the name defines the name of the tool whose benchmarking time this file contains. Note, that this part of the file name is used for plot style chosing in graphs.

## Output files

If `--save` or `--plotly` option is specified then the script creates the folder `/tmp/graphs` where it stores result plots. This directory structure has the following form:

```
/tmp/graphs
    /static
        /<BuildType>
            /jacobain
                PNG files
            /jacobian ÷ objective
                PNG files
            /objective
                PNG files
            /objective ÷ Manual
                PNG files
        /<BuildType>
            ...
    /plotly
        /<BuildType>
            /jacobain
                HTML files
            /jacobian ÷ objective
                HTML files
            /objective
                HTML files
            /objective ÷ Manual
                HTML files
        /<BuildType>
            ...
    graphs_index.json
```

Here `<BuildType>` is the build type of the run that the global runner has performed. If `--save` or `--plotly` is not specified then the respective subfolder will not be created. The following tabel describes graph types:

| Graph type | What benchmark time it shows |
| -- | -- |
| *jacobian* | Jacobian calculation time |
| *jacobian ÷ objective* | Ratio of jacobian calculation time and objective calculation time |
| *objective* | Objective calculation time |
| *objective ÷ Manual* | Ratio of tool objective calculation time and manual objective calculation time |

Names of the files in the graph type directories have the form

```
<Objective> <Size> [<GraphType>] - <BuildType> Graph.<Extension>
```

Here `<Objective>` is an objective type in the upper case (e.g. `BA`), `<GraphType>` is a graph type with the first capital letter (e.g. `Jacobian`), `<BuildType>` is the run build type, `<Extension>` is `png` or `html` respectively for static or plotly graph versions. `<Size>` contains info about the tool run benchmark size. Thus, for the _Hand_ objective size can be _simple_ or _complicated_ and _big_ or _small_, for the _GMM_ objective it can be _1k_ or _10k_. This part of the file name can be missed (e.g. the `BA` objective has not different sizes). Information about run benchmark sizes for all graph types is stored in the file `graphs_index.json`. This file has the following form:

```
{
    <BuildType>: {
        <GraphType>: {
            <Objective>: [ <Sizes> ],
            ...
        },
        ...
    },
    ...
}
```

Here `<BuildType>` is the run build type, `<GraphType>` is a graph type, `<Objective>` is an objective type in the upper case, and `<Sizes>` is an array of all run benchmark sizes for the specific objective type.

## Plot styles

Each tool has the unique specific plot style in all graphs. These styles are defined in the variable `tool_styles` in the file `/ADBench/plot_graphs.py`. This variable has the following form:

```python
tool_styles = {
    <ToolName>: ( <LineColor>, <Marker>, <DisplayName> ),
    ...
}
```

Here `<ToolName>` is the name of the tool that is extracted form the time file (see [input files](#Input-files) section), `<LineColor>` and `<Marker>` define a plot [line color](https://matplotlib.org/2.0.2/api/colors_api.html) and a [marker style](https://matplotlib.org/2.1.2/api/markers_api.html#module-matplotlib.markers), and `<DisplayName>` is a name that is shown in a graph legend (it can be not specified, in such a case `<ToolName>` is used as a display name). If you want to add a new style, you should just add a new element to this dictionary. Note, that the new style must be unique. Style display name is suggested to have the form `<Language/Platform>, <ToolName>`, where `<Language/Platform>` is a programming language or a platform of the tool, `<ToolName>` is a tool name.

If for some tools there are not specified styles then default styles are used. Default styles have no display names and their markers are crosses. Note, that default styles are not stable. That means that default style is not linked to the tool, so the tool plot can have different colors in different graphs. 